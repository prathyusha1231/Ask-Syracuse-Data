{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ask Syracuse Data: Full EDA (All 16 Datasets)\n\nComprehensive exploratory data analysis of every dataset in the project.  \nFor each dataset we look at: shape, dtypes, nulls, unique values, distributions, and what queries/analysis are possible."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "from pipeline import data_utils\n",
    "print('Imports ready')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Dataset Profile Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def profile_dataset(df, name):\n    \"\"\"Print a full profile of a dataset.\"\"\"\n    print(f\"{'='*70}\")\n    print(f\"  {name.upper()}\")\n    print(f\"{'='*70}\")\n    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n    print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n    print()\n    \n    # Column types\n    print(\"COLUMNS & TYPES:\")\n    for col in df.columns:\n        nulls = df[col].isna().sum()\n        null_pct = nulls / len(df) * 100\n        nunique = df[col].nunique()\n        print(f\"  {col:30s} {str(df[col].dtype):15s} {nunique:>6} unique  {nulls:>6} nulls ({null_pct:.1f}%)\")\n    print()\n    \n    # Numeric columns summary\n    num_cols = df.select_dtypes(include='number').columns.tolist()\n    if num_cols:\n        print(\"NUMERIC SUMMARY:\")\n        display(df[num_cols].describe())\n    \n    # Categorical columns - top values\n    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    if cat_cols:\n        print(\"\\nTOP VALUES (categorical columns):\")\n        for col in cat_cols:\n            if df[col].nunique() <= 30:\n                print(f\"\\n  {col} ({df[col].nunique()} unique):\")\n                vc = df[col].value_counts().head(10)\n                for val, cnt in vc.items():\n                    print(f\"    {str(val):40s} {cnt:>6,} ({cnt/len(df)*100:.1f}%)\")\n            else:\n                print(f\"\\n  {col}: {df[col].nunique()} unique (top 5: {df[col].value_counts().head(5).index.tolist()})\")\n    \n    # Date columns\n    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n    if date_cols:\n        print(\"\\nDATE RANGES:\")\n        for col in date_cols:\n            print(f\"  {col}: {df[col].min()} to {df[col].max()}\")\n    \n    # Year column if exists\n    if 'year' in df.columns:\n        print(f\"\\nYEAR DISTRIBUTION:\")\n        vc = df['year'].value_counts().sort_index()\n        for yr, cnt in vc.items():\n            print(f\"  {yr}: {cnt:>6,} rows\")\n    \n    print(f\"\\n{'-'*70}\\n\")\n    return {\n        'name': name,\n        'rows': len(df),\n        'cols': len(df.columns),\n        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n        'columns': list(df.columns),\n        'null_pct': {col: df[col].isna().mean()*100 for col in df.columns},\n    }",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Code Violations (~44K rows)\n",
    "Housing code enforcement records (2017-present). Key dataset for the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "violations = data_utils.load_code_violations()\n",
    "prof_violations = profile_dataset(violations, 'Code Violations')\n",
    "violations.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Violations deep-dive: status distribution, time trends, geographic spread\n",
    "print(\"Status breakdown:\")\n",
    "print(violations['status_type_name'].value_counts())\n",
    "print(f\"\\nNeighborhoods with most violations (top 10):\")\n",
    "print(violations['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nZIP codes:\")\n",
    "if 'complaint_zip' in violations.columns:\n",
    "    print(violations['complaint_zip'].value_counts().head(10))\n",
    "print(f\"\\nDate range: {violations.get('violation_date', pd.Series()).min()} to {violations.get('violation_date', pd.Series()).max()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Crime Data (~32.8K rows)\n",
    "Part 1 & Part 2 offenses, 2022-2025. ZIP derived from lat/long."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "crime = data_utils.load_crime()\n",
    "prof_crime = profile_dataset(crime, 'Crime')\n",
    "crime.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crime deep-dive\n",
    "print(\"Crime Part distribution:\")\n",
    "print(crime['crime_part'].value_counts())\n",
    "print(f\"\\nTop crime types:\")\n",
    "print(crime['code_defined'].value_counts().head(15))\n",
    "print(f\"\\nCrimes by year:\")\n",
    "print(crime['year'].value_counts().sort_index())\n",
    "print(f\"\\nNeighborhoods with most crime (top 10):\")\n",
    "print(crime['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nArrest rate:\")\n",
    "print(crime['arrest'].value_counts())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Rental Registry (~13K rows)\n",
    "Registered rental property inspections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rentals = data_utils.load_rental_registry()\n",
    "prof_rentals = profile_dataset(rentals, 'Rental Registry')\n",
    "rentals.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rental deep-dive\n",
    "print(\"Completion types:\")\n",
    "print(rentals['completion_type_name'].value_counts())\n",
    "print(f\"\\nTop ZIP codes:\")\n",
    "if 'zip' in rentals.columns:\n",
    "    print(rentals['zip'].value_counts().head(10))\n",
    "print(f\"\\nUnique SBLs (properties): {rentals['sbl'].nunique()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vacant Properties (~1.4K rows)\n",
    "Administratively identified vacant properties."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vacant = data_utils.load_vacant_properties()\n",
    "prof_vacant = profile_dataset(vacant, 'Vacant Properties')\n",
    "vacant.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vacant deep-dive\n",
    "print(\"By neighborhood (top 10):\")\n",
    "print(vacant['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nBy ZIP:\")\n",
    "if 'zip' in vacant.columns:\n",
    "    print(vacant['zip'].value_counts().head(10))\n",
    "print(f\"\\nVPR Valid:\")\n",
    "if 'vpr_valid' in vacant.columns:\n",
    "    print(vacant['vpr_valid'].value_counts())\n",
    "print(f\"\\nVPR Result:\")\n",
    "if 'vpr_result' in vacant.columns:\n",
    "    print(vacant['vpr_result'].value_counts())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Unfit Properties (353 rows)\n",
    "Properties declared unfit for habitation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unfit = data_utils.load_unfit_properties()\n",
    "prof_unfit = profile_dataset(unfit, 'Unfit Properties')\n",
    "unfit.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Trash Pickup (~41K rows)\n",
    "Collection schedules for 2025."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trash = data_utils.load_trash_pickup()\n",
    "prof_trash = profile_dataset(trash, 'Trash Pickup')\n",
    "trash.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Historical Properties (3,486 rows)\n",
    "Landmark and National Register eligible properties."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "historical = data_utils.load_historical_properties()\n",
    "prof_historical = profile_dataset(historical, 'Historical Properties')\n",
    "historical.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Historical deep-dive\n",
    "print(\"National Register eligible:\")\n",
    "if 'nr_eligible' in historical.columns:\n",
    "    print(historical['nr_eligible'].value_counts())\n",
    "print(f\"\\nLPSS (Local Protected Sites):\")\n",
    "if 'lpss' in historical.columns:\n",
    "    print(historical['lpss'].value_counts())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Assessment Roll (~41K rows)\n",
    "Property assessments and classifications (2026)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "assessment = data_utils.load_assessment_roll()\n",
    "prof_assessment = profile_dataset(assessment, 'Assessment Roll')\n",
    "assessment.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assessment deep-dive\n",
    "print(\"Property class distribution:\")\n",
    "print(assessment['prop_class_description'].value_counts().head(15))\n",
    "if 'total_av' in assessment.columns:\n",
    "    print(f\"\\nTotal assessed value stats:\")\n",
    "    print(assessment['total_av'].describe())\n",
    "    print(f\"\\nAvg assessment by top property classes:\")\n",
    "    top = assessment.groupby('prop_class_description')['total_av'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)\n",
    "    top['mean'] = top['mean'].map('${:,.0f}'.format)\n",
    "    print(top)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. SYRCityline Requests (~116K rows)\n",
    "311 service requests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cityline = data_utils.load_cityline_requests()\n",
    "prof_cityline = profile_dataset(cityline, 'SYRCityline Requests')\n",
    "cityline.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cityline deep-dive\n",
    "print(\"Top categories:\")\n",
    "print(cityline['category'].value_counts().head(15))\n",
    "print(f\"\\nTop agencies:\")\n",
    "print(cityline['agency_name'].value_counts().head(10))\n",
    "print(f\"\\nReport sources:\")\n",
    "if 'report_source' in cityline.columns:\n",
    "    print(cityline['report_source'].value_counts())\n",
    "if 'minutes_to_close' in cityline.columns:\n",
    "    print(f\"\\nMinutes to close:\")\n",
    "    print(cityline['minutes_to_close'].describe())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Snow Routes (3,685 rows)\n",
    "Emergency snow route road segments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snow = data_utils.load_snow_routes()\n",
    "prof_snow = profile_dataset(snow, 'Snow Routes')\n",
    "snow.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Bike Suitability (868 rows)\n",
    "Road bike suitability ratings (2020)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bike_suit = data_utils.load_bike_suitability()\n",
    "prof_bike_suit = profile_dataset(bike_suit, 'Bike Suitability')\n",
    "bike_suit.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Bike Infrastructure (59 rows)\n",
    "Bike lanes, trails, and paths (2023)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bike_infra = data_utils.load_bike_infrastructure()\n",
    "prof_bike_infra = profile_dataset(bike_infra, 'Bike Infrastructure')\n",
    "bike_infra.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Parking Violations (~197K rows)\n",
    "Parking tickets issued (2023). Largest dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "parking = data_utils.load_parking_violations()\n",
    "prof_parking = profile_dataset(parking, 'Parking Violations')\n",
    "parking.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parking deep-dive\n",
    "print(\"Top violation types:\")\n",
    "print(parking['description'].value_counts().head(15))\n",
    "print(f\"\\nStatus:\")\n",
    "print(parking['status'].value_counts())\n",
    "if 'amount' in parking.columns:\n",
    "    print(f\"\\nFine amounts:\")\n",
    "    print(parking['amount'].describe())\n",
    "    print(f\"\\nAvg fine by top violation types:\")\n",
    "    avg_fines = parking.groupby('description')['amount'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)\n",
    "    avg_fines['mean'] = avg_fines['mean'].map('${:.2f}'.format)\n",
    "    print(avg_fines)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Permit Requests (~47K rows)\n",
    "Building permit applications."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "permits = data_utils.load_permit_requests()\n",
    "prof_permits = profile_dataset(permits, 'Permit Requests')\n",
    "permits.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Permits deep-dive\n",
    "print(\"Permit types:\")\n",
    "print(permits['permit_type'].value_counts().head(15))\n",
    "if 'zip' in permits.columns:\n",
    "    print(f\"\\nTop ZIP codes:\")\n",
    "    print(permits['zip'].value_counts().head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Tree Inventory (~55K rows)\n",
    "City-managed tree inventory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trees = data_utils.load_tree_inventory()\n",
    "prof_trees = profile_dataset(trees, 'Tree Inventory')\n",
    "trees.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Trees deep-dive\nprint(\"NOTE: Tree inventory has NO 'condition' column (schema references it but it doesn't exist)\")\nprint(\"'area' column = neighborhood equivalent\\n\")\nprint(f\"Top species:\")\nif 'spp_com' in trees.columns:\n    print(trees['spp_com'].value_counts().head(15))\nprint(f\"\\nTrees by area/neighborhood (top 10):\")\nif 'area' in trees.columns:\n    print(trees['area'].value_counts().head(10))\nprint(f\"\\nARPA (American Rescue Plan Act) trees:\")\nif 'arpa' in trees.columns:\n    print(trees['arpa'].value_counts())\nprint(f\"\\nDiameter (DBH) stats:\")\nif 'dbh' in trees.columns:\n    print(trees['dbh'].describe())\nprint(f\"\\nCensus tracts: {trees['censustrac'].nunique() if 'censustrac' in trees.columns else 'N/A'}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Lead Testing (1,185 rows)\n",
    "Elevated lead levels by census tract (2013-2024)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lead = data_utils.load_lead_testing()\n",
    "prof_lead = profile_dataset(lead, 'Lead Testing')\n",
    "lead.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Lead deep-dive\n",
    "print(\"Year distribution:\")\n",
    "print(lead['year'].value_counts().sort_index())\n",
    "print(f\"\\nCensus tracts: {lead['census_tract'].nunique()}\")\n",
    "if 'pct_elevated' in lead.columns:\n",
    "    print(f\"\\nPercent elevated stats:\")\n",
    "    print(lead['pct_elevated'].describe())\n",
    "    print(f\"\\nTracts with highest avg elevated lead:\")\n",
    "    top_lead = lead.groupby('census_tract')['pct_elevated'].mean().sort_values(ascending=False).head(10)\n",
    "    print(top_lead)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: All Datasets at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build summary table\n",
    "all_datasets = {\n",
    "    'Code Violations': violations,\n",
    "    'Crime': crime,\n",
    "    'Rental Registry': rentals,\n",
    "    'Vacant Properties': vacant,\n",
    "    'Unfit Properties': unfit,\n",
    "    'Trash Pickup': trash,\n",
    "    'Historical Properties': historical,\n",
    "    'Assessment Roll': assessment,\n",
    "    'Cityline Requests': cityline,\n",
    "    'Snow Routes': snow,\n",
    "    'Bike Suitability': bike_suit,\n",
    "    'Bike Infrastructure': bike_infra,\n",
    "    'Parking Violations': parking,\n",
    "    'Permit Requests': permits,\n",
    "    'Tree Inventory': trees,\n",
    "    'Lead Testing': lead,\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, df in all_datasets.items():\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n",
    "    has_geo = any(c in df.columns for c in ['latitude', 'longitude', 'lat', 'long', 'x', 'y'])\n",
    "    has_zip = 'zip' in df.columns or 'complaint_zip' in df.columns\n",
    "    has_neighborhood = 'neighborhood' in df.columns\n",
    "    null_pct = df.isna().mean().mean() * 100\n",
    "    \n",
    "    summary.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': f\"{len(df):,}\",\n",
    "        'Cols': len(df.columns),\n",
    "        'Memory (MB)': f\"{df.memory_usage(deep=True).sum()/1024**2:.1f}\",\n",
    "        'Numeric Cols': len(num_cols),\n",
    "        'Date Cols': len(date_cols),\n",
    "        'Has Geo': '✓' if has_geo else '',\n",
    "        'Has ZIP': '✓' if has_zip else '',\n",
    "        'Has Neighborhood': '✓' if has_neighborhood else '',\n",
    "        'Avg Null %': f\"{null_pct:.1f}%\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"ALL 16 DATASETS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_df)\n",
    "\n",
    "total_rows = sum(len(df) for df in all_datasets.values())\n",
    "total_mem = sum(df.memory_usage(deep=True).sum() for df in all_datasets.values()) / 1024**2\n",
    "print(f\"\\nTotal: {total_rows:,} rows across 16 datasets, {total_mem:.1f} MB in memory\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross-Dataset Join Potential\n",
    "Which datasets can be linked together and how?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check join keys across datasets\nprint(\"JOIN KEY ANALYSIS\")\nprint(\"=\" * 70)\n\n# SBL joins (property-level)\nsbl_datasets = {name: df for name, df in all_datasets.items() if 'sbl' in df.columns}\nprint(f\"\\nDatasets with SBL (property ID), enables property-level joins:\")\nfor name, df in sbl_datasets.items():\n    print(f\"  {name}: {df['sbl'].nunique():,} unique SBLs\")\n\n# ZIP joins\nzip_datasets = {}\nfor name, df in all_datasets.items():\n    if 'zip' in df.columns:\n        zip_datasets[name] = df['zip']\n    elif 'complaint_zip' in df.columns:\n        zip_datasets[name] = df['complaint_zip']\nprint(f\"\\nDatasets with ZIP code, enables area-level joins:\")\nfor name, series in zip_datasets.items():\n    print(f\"  {name}: {series.nunique()} unique ZIPs\")\n\n# Neighborhood joins\nnbhd_datasets = {name: df for name, df in all_datasets.items() if 'neighborhood' in df.columns}\nprint(f\"\\nDatasets with Neighborhood, enables neighborhood-level joins:\")\nfor name, df in nbhd_datasets.items():\n    print(f\"  {name}: {df['neighborhood'].nunique()} unique neighborhoods\")\n\n# Check neighborhood overlap\nif len(nbhd_datasets) >= 2:\n    names = list(nbhd_datasets.keys())\n    print(f\"\\nNeighborhood overlap between {names[0]} and {names[1]}:\")\n    set1 = set(nbhd_datasets[names[0]]['neighborhood'].dropna().unique())\n    set2 = set(nbhd_datasets[names[1]]['neighborhood'].dropna().unique())\n    overlap = set1 & set2\n    print(f\"  {names[0]}: {len(set1)} | {names[1]}: {len(set2)} | Overlap: {len(overlap)}\")\n    only1 = set1 - set2\n    only2 = set2 - set1\n    if only1:\n        print(f\"  Only in {names[0]}: {sorted(only1)[:10]}\")\n    if only2:\n        print(f\"  Only in {names[1]}: {sorted(only2)[:10]}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Comprehensive data quality check across all datasets\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for name, df in all_datasets.items():\n",
    "    # Check for duplicate rows\n",
    "    dupes = df.duplicated().sum()\n",
    "    if dupes > 0:\n",
    "        quality_issues.append(f\"  {name}: {dupes:,} duplicate rows ({dupes/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for high-null columns (>50%)\n",
    "    for col in df.columns:\n",
    "        null_pct = df[col].isna().mean() * 100\n",
    "        if null_pct > 50:\n",
    "            quality_issues.append(f\"  {name}.{col}: {null_pct:.0f}% null\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(\"\\nIssues found:\")\n",
    "    for issue in quality_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"  No major quality issues found!\")\n",
    "\n",
    "# Check for common column name inconsistencies\n",
    "print(\"\\nCOLUMN NAME CONSISTENCY:\")\n",
    "all_cols = set()\n",
    "for name, df in all_datasets.items():\n",
    "    all_cols.update(df.columns)\n",
    "# Check for near-duplicates\n",
    "col_list = sorted(all_cols)\n",
    "print(f\"  Total unique columns across all datasets: {len(col_list)}\")\n",
    "print(f\"  Geographic columns found: {[c for c in col_list if c in ['zip', 'complaint_zip', 'latitude', 'longitude', 'lat', 'long', 'x', 'y', 'neighborhood', 'census_tract']]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# What Can We Do With This Data?\n\n## Currently Supported Queries\n- Counts, averages, min/max/sum by group\n- Temporal breakdowns (year, month, quarter)\n- Cross-dataset joins (violations + rentals, crime + vacant, etc.)\n- Rankings, percentiles via LLM SQL path\n\n## New Analysis Opportunities"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Identify new analysis opportunities based on the data\n\nprint(\"ANALYSIS OPPORTUNITIES\")\nprint(\"=\" * 70)\n\nprint(\"\"\"\n1. HOUSING HEALTH INDEX\n   Combine: violations + vacant + unfit + rental_registry + assessment\n   By: neighborhood or ZIP\n   Metric: composite score of property distress\n   Data available: All 5 datasets have ZIP or neighborhood\n\n2. CRIME + PROPERTY CONDITIONS CORRELATION\n   Combine: crime + violations + vacant_properties\n   Question: \"Do neighborhoods with more vacant properties have more crime?\"\n   Data available: All have neighborhood column\n\n3. CITY RESPONSIVENESS ANALYSIS\n   Dataset: cityline_requests (minutes_to_close)\n   Questions: Which categories get resolved fastest? Which ZIPs wait longest?\n   Data available: 116K requests with response times\n\n4. LEAD EXPOSURE + HOUSING CONDITIONS\n   Combine: lead_testing + violations (would need census tract to neighborhood/ZIP mapping)\n   Question: \"Do areas with more code violations have higher lead levels?\"\n   Gap: lead_testing uses census_tract, others use ZIP/neighborhood\n\n5. TEMPORAL TRENDS (Multi-Year)\n   Datasets with year: crime (2022-2025), violations (2017+), cityline, permits, parking\n   Questions: \"Is crime going up or down?\" \"Violation trends over time?\"\n   Data available: Yes\n\n6. TREE CANOPY + NEIGHBORHOOD QUALITY\n   Combine: tree_inventory + violations + crime\n   Question: \"Do neighborhoods with more trees have fewer violations?\"\n   Data available: All have neighborhood\n\n7. PROPERTY VALUE ANALYSIS\n   Dataset: assessment_roll (total_av by prop_class, ZIP)\n   Combined with: violations, vacant to see if distressed areas have lower assessments\n   Data available: Yes\n\n8. GEOGRAPHIC HOTSPOT MAPPING\n   Datasets with lat/long: crime, cityline, parking_violations\n   Opportunity: heatmaps, clustering, density analysis\n   Currently: only bubble maps and point maps\n\"\"\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Column Inventory (Every Column in Every Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Full column inventory for reference\n",
    "print(\"FULL COLUMN INVENTORY\")\n",
    "print(\"=\" * 70)\n",
    "for name, df in all_datasets.items():\n",
    "    print(f\"\\n{name} ({len(df):,} rows):\")\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        nunique = df[col].nunique()\n",
    "        sample = str(df[col].dropna().iloc[0])[:50] if df[col].notna().any() else 'ALL NULL'\n",
    "        print(f\"  {col:35s} {dtype:15s} {nunique:>6} unique  sample: {sample}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# EDA Findings & Inconsistencies\n\n## Dataset Overview (691,827 total rows, 387.6 MB in memory)\n\n| Dataset | Rows | Cols | Geo | ZIP | Neighborhood | SBL | Year |\n|---|---:|---:|:---:|:---:|:---:|:---:|:---:|\n| Code Violations | 137,663 | 25 | Y | Y | Y | Y | |\n| Crime | 32,840 | 17 | Y | Y | Y | | Y |\n| Rental Registry | 11,085 | 22 | Y | Y | | Y | |\n| Vacant Properties | 1,651 | 17 | Y | Y | Y | Y | |\n| Unfit Properties | 353 | 27 | Y | Y | | Y | |\n| Trash Pickup | 41,096 | 7 | | Y | | Y | |\n| Historical Properties | 3,486 | 7 | Y | Y | | Y | |\n| Assessment Roll | 41,367 | 27 | | Y | | Y | |\n| Cityline Requests | 116,143 | 19 | Y | Y | | | Y |\n| Snow Routes | 3,685 | 70 | | Y | | | |\n| Bike Suitability | 868 | 2 | | | | | |\n| Bike Infrastructure | 59 | 4 | | | | | |\n| Parking Violations | 196,768 | 9 | Y | Y | | | Y |\n| Permit Requests | 47,902 | 9 | Y | Y | | | Y |\n| Tree Inventory | 55,676 | 15 | Y | Y | | | |\n| Lead Testing | 1,185 | 3 | | | | | Y |\n\n---\n\n## SBL (Property-Level) Join Overlap\n\n7 datasets share SBL. Strongest overlaps:\n\n| Dataset A | Dataset B | Shared SBLs | A total | B total | Match % (of smaller) |\n|---|---|---:|---:|---:|---:|\n| Code Violations | Assessment Roll | 17,074 | 17,348 | 41,367 | 98.4% |\n| Code Violations | Trash Pickup | 17,192 | 17,348 | 40,865 | 99.1% |\n| Code Violations | Rental Registry | 7,991 | 17,348 | 11,048 | 72.3% |\n| Rental Registry | Assessment Roll | 11,028 | 11,048 | 41,367 | 99.8% |\n| Code Violations | Vacant Properties | 1,483 | 17,348 | 1,651 | 89.8% |\n| Vacant Properties | Assessment Roll | 1,632 | 1,651 | 41,367 | 98.8% |\n| Vacant Properties | Unfit Properties | 101 | 1,651 | 305 | 33.1% |\n| Trash Pickup | Assessment Roll | 40,535 | 40,865 | 41,367 | 99.2% |\n| Historical Properties | Assessment Roll | 3,439 | 3,471 | 41,367 | 99.1% |\n\nAssessment Roll is the universal backbone -- nearly every property in other datasets matches it.\n\n## Neighborhood Join Overlap\n\nOnly 3 datasets have `neighborhood`: Code Violations (34), Crime (34), Vacant Properties (33).\n\n- Violations + Crime: 27/41 neighborhoods match\n- Violations + Vacant: 33/34 match\n- Crime + Vacant: 27/40 match\n\nTree Inventory uses `area` instead of `neighborhood` (same concept, 31 areas). Schema already correctly uses `area`.\n\n## ZIP Codes\n\n12 Syracuse ZIPs across 12/16 datasets: 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13210, 13214, 13215, 13219, 13224.\n\nDatasets without ZIP: Bike Suitability, Bike Infrastructure, Lead Testing (uses census_tract).\n\n---\n\n## Data Quality Issues Found\n\n### Duplicate Rows (FIXED)\n- **Trash Pickup**: 221 duplicate rows (0.5%) -- FIXED: added `.drop_duplicates()` in loader\n- **Cityline Requests**: 835 duplicate rows (0.7%) -- FIXED: added `.drop_duplicates()` in loader\n- **Historical Properties**: 15 duplicate rows (0.4%) -- FIXED: added `.drop_duplicates()` in loader\n- **Permit Requests**: 1 duplicate row (negligible, not fixed)\n\n### High-Null Columns (>50%)\n\n| Dataset | Column | Null % | Notes |\n|---|---|---:|---|\n| Code Violations | `status_date` | 88.6% | Only populated for closed violations |\n| Code Violations | `vacant` | 86.2% | Only \"Residential\"/\"Commercial\" when flagged |\n| Crime | `arrest` | 86.9% | Was only \"Yes\" values -- FIXED: nulls now filled with \"No\" |\n| Crime | `larcenycode` | 53.7% | Only applies to larceny offenses |\n| Crime | `qualityoflife` | 59.2% | Only for 2023+ data |\n| Rental Registry | `rr_ext_insp_fail` | 94.6% | Only populated on failure |\n| Rental Registry | `rr_int_insp_fail` | 92.2% | Only populated on failure |\n| Rental Registry | `rr_ext_insp_pass` | 56.0% | Only populated on pass |\n| Rental Registry | `rr_int_insp_pass` | 59.1% | Only populated on pass |\n| Rental Registry | `shape` | 100.0% | FIXED: dropped in loader (GIS artifact) |\n| Vacant Properties | `completion_date` | 77.3% | Only for VPR-certified properties |\n| Unfit Properties | `vacant` | 100.0% | FIXED: dropped in loader |\n| Assessment Roll | `secondary_owner` | 96.3% | Rare |\n| Assessment Roll | `po_box` | 96.2% | Rare |\n| Assessment Roll | `exemption_1_*` through `exemption_6_*` | 67-100% | Most properties have 0-1 exemptions |\n| Cityline Requests | `acknowledged_at_local` | 80.6% | Most requests never formally acknowledged |\n| Cityline Requests | `minutes_to_acknowledge` | 80.6% | Same as above |\n\n### Schema vs Reality Mismatches\n\n| Issue | Status |\n|---|---|\n| Tree `condition`/`neighborhood` columns | NOT A BUG: schema.py already correctly uses `area` and `dbh`. Only CLAUDE.md was wrong -- FIXED in CLAUDE.md. |\n| Unfit `vacant` column is 100% null | FIXED: column dropped in loader |\n| Rental Registry `shape` column is 100% null | FIXED: column dropped in loader |\n| Crime `arrest` has no \"No\" values | FIXED: null handling changed from \"Unknown\" to \"No\" |\n| Code Violations actual rows: 137,663 | FIXED: README updated from \"~44K\" to \"~138K\" |\n| Cityline/Parking/Permits missing extracted `year` column | FIXED: year extracted from date columns in all 3 loaders |\n\n---\n\n## Unexposed Columns Worth Adding\n\n### High Value (would enable new query types)\n\n| Dataset | Column | Values | Enables |\n|---|---|---|---|\n| Crime | `larcenycode` | 12 types (All Other, From Building, Shoplifting, etc.) | \"What types of larceny are most common?\" |\n| Crime | `arrest` | Yes (4,315) / No (28,525) | \"How many crimes resulted in arrest?\" |\n| Cityline | `sla_in_hours` | 17 values (0-2605 hrs) | \"Which request types have the longest SLA?\" |\n| Cityline | `summary` | 266 types | More granular than `category` |\n| Tree Inventory | `censustrac` | 56 census tracts | Could link to lead testing data |\n| Tree Inventory | `arpa` | Yes (55.8%) / No (44.2%) | \"How many ARPA-funded trees?\" |\n| Assessment Roll | `school_taxable` | Varies by exemptions | Tax analysis |\n\n### Medium Value\n\n| Dataset | Column | Notes |\n|---|---|---|\n| Crime | `qualityoflife` | True/False, but 59% null (only 2023+ data) |\n| Vacant Properties | `vacant` | Residential (90.2%) vs Commercial (9.8%) |\n| Rental Registry | `rrisvalid` | Yes (40.8%) / No (59.2%) -- current validity |\n| Unfit Properties | `corrective_action` | 249 unique text descriptions |\n| Parking Violations | `location` | 8,874 unique locations |\n| Permit Requests | `description_of_work` | 29,283 unique -- free text, hard to group |\n\n---\n\n## New Cross-Dataset Analysis Opportunities\n\n1. **Housing Health Index** -- Combine violations + vacant + unfit + rental_registry + assessment by neighborhood/ZIP into a composite distress score. All 5 datasets share SBL or ZIP.\n\n2. **Crime vs Property Conditions** -- Do neighborhoods with more vacant properties have more crime? All 3 have neighborhood. 27/41 neighborhoods overlap between crime and violations.\n\n3. **City Responsiveness** -- Cityline has 116K requests with `minutes_to_close`. Which categories resolve fastest? Which ZIPs wait longest? Median close time is 1,327 minutes (~22 hours), max is 1,408,207 minutes (~2.7 years).\n\n4. **Lead Exposure vs Housing** -- Lead testing uses census_tract (157 tracts). Tree inventory also has `censustrac` (56 tracts). Could bridge lead data to property data through census tracts.\n\n5. **Tree Canopy vs Neighborhood Quality** -- Tree inventory `area` column maps to neighborhoods. Compare tree density/diameter against violations and crime rates.\n\n6. **Property Value vs Distress** -- Assessment roll has property values for 98.8% of vacant properties (via SBL). Compare assessed values for vacant vs non-vacant, violation-heavy vs clean.\n\n7. **Temporal Trends** -- Crime (2022-2025), violations (2017+), permits, cityline all have date columns. Year-over-year trend analysis. Note: 2025 crime data is only 33 rows (Jan 1-5).\n\n8. **Geographic Hotspot Mapping** -- 9 datasets have lat/long. Density heatmaps, clustering, hotspot detection. Currently only bubble and point maps in the app.\n\n---\n\n## All Fixes Applied\n\n| # | Fix | What Changed | File(s) Modified |\n|---|---|---|---|\n| 1 | Pre-merged crime CSV | Created `crime_merged.csv` (32,840 rows) with ZIP, neighborhood, year, crime_part pre-computed. Eliminates geocoding on every load. 1.8x faster. | `data/raw/crime_merged.csv` |\n| 2 | `load_crime()` uses merged file | Fast path reads single CSV. Falls back to 6-CSV + geocoding if missing. Also applies null handling to merged path. | `pipeline/data_utils.py` |\n| 3 | README violation count | Changed \"~44K\" to \"~138K\" (137,663 violation rows). Also fixed vacant from \"~1.4K\" to \"~1.7K\". | `README.md` |\n| 4 | Trash Pickup duplicates | Added `.drop_duplicates()` in loader. Removes 221 duplicate rows (41,096 -> 40,875). | `pipeline/data_utils.py` |\n| 5 | Cityline duplicates | Added `.drop_duplicates()` in loader. Removes 835 duplicate rows (116,143 -> 115,308). | `pipeline/data_utils.py` |\n| 6 | Historical Properties duplicates | Added `.drop_duplicates()` in loader. Removes 15 duplicate rows (3,486 -> 3,471). | `pipeline/data_utils.py` |\n| 7 | Rental Registry `shape` column | Dropped 100% null column in loader. Saves memory. 22 -> 21 columns. | `pipeline/data_utils.py` |\n| 8 | Unfit Properties `vacant` column | Dropped 100% null column in loader. 27 -> 26 columns. | `pipeline/data_utils.py` |\n| 9 | Crime `arrest` null values | Changed null handling label from \"Unknown\" to \"No\". Now shows Yes (4,315) / No (28,525) instead of Yes/null. Regenerated crime_merged.csv. | `pipeline/data_utils.py`, `data/raw/crime_merged.csv` |\n| 10 | Year extraction for 3 datasets | Added `year = dt.year` extraction in loaders for Cityline (from `created_at_local`, 2021-2025), Parking Violations (from `issued_date`, 2023-2025), and Permit Requests (from `issue_date`, 1980-2025). Schema already had `year` in `temporal_group_map` but the column didn't exist at load time. | `pipeline/data_utils.py` |\n| 11 | CLAUDE.md tree_inventory entry | Fixed \"neighborhood, condition\" to \"area, spp_com, zip; avg/min/max dbh\" to match actual schema and data. | `CLAUDE.md` |\n\nAll 13 benchmark tests pass after fixes (100%).",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}