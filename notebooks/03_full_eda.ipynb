{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Syracuse Data: Full EDA (All 16 Datasets)\n",
    "\n",
    "Comprehensive exploratory data analysis of every dataset in the project.  \n",
    "For each dataset we look at: shape, dtypes, nulls, unique values, distributions, and what queries/analysis are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "from pipeline import data_utils\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Dataset Profile Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataset(df, name):\n",
    "    \"\"\"Print a full profile of a dataset.\"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  {name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    # Column types\n",
    "    print(\"COLUMNS & TYPES:\")\n",
    "    for col in df.columns:\n",
    "        nulls = df[col].isna().sum()\n",
    "        null_pct = nulls / len(df) * 100\n",
    "        nunique = df[col].nunique()\n",
    "        print(f\"  {col:30s} {str(df[col].dtype):15s} {nunique:>6} unique  {nulls:>6} nulls ({null_pct:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    if num_cols:\n",
    "        print(\"NUMERIC SUMMARY:\")\n",
    "        display(df[num_cols].describe())\n",
    "    \n",
    "    # Categorical columns - top values\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if cat_cols:\n",
    "        print(\"\\nTOP VALUES (categorical columns):\")\n",
    "        for col in cat_cols:\n",
    "            if df[col].nunique() <= 30:\n",
    "                print(f\"\\n  {col} ({df[col].nunique()} unique):\")\n",
    "                vc = df[col].value_counts().head(10)\n",
    "                for val, cnt in vc.items():\n",
    "                    print(f\"    {str(val):40s} {cnt:>6,} ({cnt/len(df)*100:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"\\n  {col}: {df[col].nunique()} unique (top 5: {df[col].value_counts().head(5).index.tolist()})\")\n",
    "    \n",
    "    # Date columns\n",
    "    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n",
    "    if date_cols:\n",
    "        print(\"\\nDATE RANGES:\")\n",
    "        for col in date_cols:\n",
    "            print(f\"  {col}: {df[col].min()} to {df[col].max()}\")\n",
    "    \n",
    "    # Year column if exists\n",
    "    if 'year' in df.columns:\n",
    "        print(f\"\\nYEAR DISTRIBUTION:\")\n",
    "        vc = df['year'].value_counts().sort_index()\n",
    "        for yr, cnt in vc.items():\n",
    "            print(f\"  {yr}: {cnt:>6,} rows\")\n",
    "    \n",
    "    print(f\"\\n{'-'*70}\\n\")\n",
    "    return {\n",
    "        'name': name,\n",
    "        'rows': len(df),\n",
    "        'cols': len(df.columns),\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'columns': list(df.columns),\n",
    "        'null_pct': {col: df[col].isna().mean()*100 for col in df.columns},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Code Violations (~138K rows)\n",
    "**Source:** Department of Neighborhood & Business Development (NBD) - Division of Code Enforcement  \n",
    "**Description:** All code violations reported to Code Enforcement (2017-present). The Division of Code Enforcement maintains Housing and Property codes and enforces compliance with the Syracuse Zoning Ordinance. When a structure is not in compliance, it can be cited for a violation. The owner has a set time to bring the structure into compliance. Cases remain open until fixed or referred to the law department.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `x`, `y` | Float | Map projection coordinates |\n",
    "| `sbl` | String | Section, Block, Lot -- property tax map number unique to this property |\n",
    "| `complaint_address` | String | Parcel address associated with the Code Enforcement case |\n",
    "| `complaint_zip` | Int | Parcel zip code associated with the Code Enforcement case |\n",
    "| `complaint_number` | String | Violation's parent complaint record case number |\n",
    "| `complaint_type_name` | String | Type of complaint filed |\n",
    "| `violation_id` | Int | Unique violation identifier |\n",
    "| `open_date` | Date | Violation's parent complaint record open date |\n",
    "| `violation_number` | String | Violation identifier string |\n",
    "| `violation` | String | The violation type -- references the specific code in violation, including the codebook name and section |\n",
    "| `violation_date` | Date | Date the violation was cited |\n",
    "| `comply_by_date` | Date | Date by which the owner must comply and address the issued code violation |\n",
    "| `status_type_name` | String | Status of the violation as of the date of extract (Open, Closed) |\n",
    "| `status_date` | Date | Date the status was changed; if \"Closed,\" reflects the date the violation was closed. ~88.6% null |\n",
    "| `issued_to` | String | Name of contact the violation was issued to (default: assessed owner at time of citation) |\n",
    "| `issued_to_address` | String | Street address of contact violation was issued to |\n",
    "| `issued_to_city` | String | City of contact violation was issued to |\n",
    "| `issued_to_zip` | String | Zip code of contact violation was issued to |\n",
    "| `inspector_id` | Int | Code Enforcement inspector identifier |\n",
    "| `neighborhood` | String | Neighborhood where the complaint address is located |\n",
    "| `vacant` | String | Is the parcel a Code Enforcement-monitored vacant structure? \"Residential\" or \"Commercial\" = Yes; Null = No. ~86.2% null |\n",
    "| `latitude` | Float | Latitude of the parcel |\n",
    "| `longitude` | Float | Longitude of the parcel |\n",
    "| `objectid` | Int | ArcGIS system field |\n",
    "\n",
    "**Computed columns (added by pipeline):** `days_to_comply`, `days_open`, `cert_duration_days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = data_utils.load_code_violations()\n",
    "prof_violations = profile_dataset(violations, 'Code Violations')\n",
    "violations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violations deep-dive: status distribution, time trends, geographic spread\n",
    "print(\"Status breakdown:\")\n",
    "print(violations['status_type_name'].value_counts())\n",
    "print(f\"\\nNeighborhoods with most violations (top 10):\")\n",
    "print(violations['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nZIP codes:\")\n",
    "if 'complaint_zip' in violations.columns:\n",
    "    print(violations['complaint_zip'].value_counts().head(10))\n",
    "print(f\"\\nDate range: {violations.get('violation_date', pd.Series()).min()} to {violations.get('violation_date', pd.Series()).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Crime Data (~32.8K rows)\n",
    "**Source:** Syracuse Police Department (SPD)  \n",
    "**Description:** Calls for service responded to by SPD (2022-2025). Excludes rape offenses and sealed cases. Addresses anonymized to the 100-block level for privacy. Crimes reported under FBI Uniform Crime Reports (UCR): **Part 1** (homicide, robbery, aggravated assault, burglary, larceny-theft, motor vehicle theft) and **Part 2** (all other offenses, quality-of-life). The date/time provided is when the crime was reported, not necessarily when it occurred. 2025 data is partial (33 rows).\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `dateend` | Datetime | Date the crime was **reported** (may have occurred earlier). Mixed formats across source CSVs |\n",
    "| `timestart` | Int/String | Start of time window in military time (2400 format). Burglaries/larcenies are often a range |\n",
    "| `timeend` | Int | End of time window in military time (2400 format) |\n",
    "| `address` | String | Where the crime occurred, anonymized to the 100-block level for resident privacy |\n",
    "| `code_defined` | String | The offense type / crime classification (e.g., \"Burglary\", \"Larceny-Theft\", \"Aggravated Assault\") |\n",
    "| `arrest` | String | Whether an arrest was made (\"Yes\" or \"No\"). Nulls filled with \"No\" by pipeline |\n",
    "| `larcenycode` | String | Larceny sub-classification (e.g., \"All Other\", \"From Building\", \"Shoplifting\"). Only for larceny offenses; ~53.7% null |\n",
    "| `qualityoflife` | String | Quality-of-life offense flag. Only in 2023+ Part 2 data; ~59.2% null |\n",
    "| `objectid` | Int | ArcGIS system field |\n",
    "| `zip` | Int (nullable) | ZIP code derived from lat/long using nearest Syracuse ZIP centroid (computed by pipeline) |\n",
    "| `neighborhood` | String | Neighborhood derived from geocoding (normalized by pipeline) |\n",
    "| `latitude` | Float | Latitude coordinate (approximate, block level) |\n",
    "| `longitude` | Float | Longitude coordinate (approximate, block level) |\n",
    "| `year` | Int | Year extracted from `dateend` |\n",
    "| `crime_part` | Int | 1 = Part 1 (serious) offense, 2 = Part 2 (other) offense |\n",
    "| `x`, `y` | Float | Map projection coordinates (from geocoding, primarily 2022 data) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = data_utils.load_crime()\n",
    "prof_crime = profile_dataset(crime, 'Crime')\n",
    "crime.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime deep-dive\n",
    "print(\"Crime Part distribution:\")\n",
    "print(crime['crime_part'].value_counts())\n",
    "print(f\"\\nTop crime types:\")\n",
    "print(crime['code_defined'].value_counts().head(15))\n",
    "print(f\"\\nCrimes by year:\")\n",
    "print(crime['year'].value_counts().sort_index())\n",
    "print(f\"\\nNeighborhoods with most crime (top 10):\")\n",
    "print(crime['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nArrest rate:\")\n",
    "print(crime['arrest'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Rental Registry (~13K rows)\n",
    "**Source:** Division of Code Enforcement (DOCE) - Periodic Inspection Program  \n",
    "**Description:** Owners of one- or two-family non-owner-occupied dwellings rented or leased within Syracuse must obtain a Rental Registry Certificate every 3 years. This dataset tracks the inspection and certification status of registered rental properties.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `x`, `y` | Float | Map projection coordinates |\n",
    "| `sbl` | String | Section, Block, Lot -- property tax number unique to this property |\n",
    "| `propertyaddress` | String | Parcel street number and street address |\n",
    "| `zip` | Int | Zip code of the parcel's street address |\n",
    "| `needsrr` | String | Whether the parcel is flagged as \"Rental Registry Required\" (1-2 family non-owner occupied) |\n",
    "| `inspect_period` | Date | Date associated with the RR case (could be case creation, upcoming inspection, or reapplication due date) |\n",
    "| `completion_type_name` | String | \"Rental Registry Card Issued\" or \"Family Based Exemption Granted\" |\n",
    "| `completion_date` | Date | Date the completion type was entered on a RR case |\n",
    "| `valid_until` | Date | Date the current RR certificate expires (3 years from last completion) |\n",
    "| `rrisvalid` | String | Whether the Rental Registry certificate is currently valid |\n",
    "| `rr_app_received` | Date | Rental Registry application received date |\n",
    "| `rr_ext_insp_pass` | Date | Exterior inspection pass date. ~56% null |\n",
    "| `rr_ext_insp_fail` | Date | Exterior inspection fail date. ~94.6% null |\n",
    "| `rr_int_insp_fail` | Date | Interior inspection fail date. ~92.2% null |\n",
    "| `rr_int_insp_pass` | Date | Interior inspection pass date. ~59.1% null |\n",
    "| `rr_contact_name` | String | Contact name for the rental property |\n",
    "| `pc_owner` | String | Property card owner name |\n",
    "| `latitude` | Float | Latitude of the parcel |\n",
    "| `longitude` | Float | Longitude of the parcel |\n",
    "| `objectid` | Int | ArcGIS system field |\n",
    "\n",
    "*Note: `shape` column (100% null GIS artifact) is dropped by the pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals = data_utils.load_rental_registry()\n",
    "prof_rentals = profile_dataset(rentals, 'Rental Registry')\n",
    "rentals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental deep-dive\n",
    "print(\"Completion types:\")\n",
    "print(rentals['completion_type_name'].value_counts())\n",
    "print(f\"\\nTop ZIP codes:\")\n",
    "if 'zip' in rentals.columns:\n",
    "    print(rentals['zip'].value_counts().head(10))\n",
    "print(f\"\\nUnique SBLs (properties): {rentals['sbl'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vacant Properties (~1.7K rows)\n",
    "**Source:** Department of Neighborhood & Business Development (NBD)  \n",
    "**Description:** Houses in Syracuse identified as vacant by NBD. Includes Vacant Property Registry (VPR) status -- whether the owner has registered and maintains the vacant property.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `x`, `y` | Float | Map projection coordinates |\n",
    "| `sbl` | String | Section, Block, Lot -- tax map ID unique to this property |\n",
    "| `propertyaddress` | String | Parcel street address |\n",
    "| `zip` | Int | Parcel zip code |\n",
    "| `owner` | String | Assessed owner name |\n",
    "| `owneraddress` | String | Assessed owner address |\n",
    "| `vacant` | String | Vacant structure type: \"Residential\" (~90.2%) or \"Commercial\" (~9.8%) |\n",
    "| `neighborhood` | String | Neighborhood the property is in |\n",
    "| `vpr_result` | String | VPR status: \"Terminated\", \"Valid - Year 1/2/3/4+\", or \"Exempt at this time\" |\n",
    "| `completion_date` | Date | VPR certificate issuance date. ~77.3% null |\n",
    "| `completion_type_name` | String | VPR completion type |\n",
    "| `valid_until` | Date | VPR certificate expiration date |\n",
    "| `vpr_valid` | String | Does property have a valid VPR certificate? (\"Y\" or \"N\") |\n",
    "| `latitude` | Float | Latitude of the property |\n",
    "| `longitude` | Float | Longitude of the property |\n",
    "| `objectid` | Int | ArcGIS system field |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant = data_utils.load_vacant_properties()\n",
    "prof_vacant = profile_dataset(vacant, 'Vacant Properties')\n",
    "vacant.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacant deep-dive\n",
    "print(\"By neighborhood (top 10):\")\n",
    "print(vacant['neighborhood'].value_counts().head(10))\n",
    "print(f\"\\nBy ZIP:\")\n",
    "if 'zip' in vacant.columns:\n",
    "    print(vacant['zip'].value_counts().head(10))\n",
    "print(f\"\\nVPR Valid:\")\n",
    "if 'vpr_valid' in vacant.columns:\n",
    "    print(vacant['vpr_valid'].value_counts())\n",
    "print(f\"\\nVPR Result:\")\n",
    "if 'vpr_result' in vacant.columns:\n",
    "    print(vacant['vpr_result'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Unfit Properties (353 rows)\n",
    "**Source:** Division of Code Enforcement (DOCE)  \n",
    "**Description:** Houses declared unfit for habitation by Code Enforcement. Process: community complaint -> inspector inspection -> violation citation -> comply-by date. Non-compliance may lead to Law Department referral, Bureau of Administrative Adjudication, or demolition pipeline.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `complaint_number` | String | Case number of violation's parent complaint record |\n",
    "| `address` | String | Street address of the property with the violation |\n",
    "| `area_involved` | String | Physical area the violation applies to, per inspector |\n",
    "| `zip` | Int | Zip code of the property |\n",
    "| `sbl` | String | Section, Block, Lot -- property tax number unique to this property |\n",
    "| `parcel_id` | Int | Property parcel number in the NYS Property database |\n",
    "| `violation` | String | Violation type -- the specific code in violation (codebook name and section) |\n",
    "| `violation_date` | Date | Date the violation was cited (stored as epoch milliseconds) |\n",
    "| `comply_by_date` | Date | Violation compliance deadline per inspector |\n",
    "| `status_type_name` | String | Violation status: Open, Closed, or Void |\n",
    "| `department_name` | String | Department the violation/complaint belongs to |\n",
    "| `status_date` | Date | Date the violation status was created or changed |\n",
    "| `complaint_type_name` | String | Violation's parent complaint type |\n",
    "| `comp_status` | String | Status of the parent complaint |\n",
    "| `comp_open_date` | Date | Date the parent complaint was opened |\n",
    "| `comp_close_date` | Date | Date the parent complaint was closed |\n",
    "| `article` | String | Article of the specific code in violation |\n",
    "| `corrective_action` | String | Corrective action required for compliance, per inspector (249 unique descriptions) |\n",
    "| `owner_name` | String | Registered owner name |\n",
    "| `owner_address` | String | Owner's street address |\n",
    "| `owner_city` | String | Owner's city |\n",
    "| `owner_state` | String | Owner's state (abbreviation) |\n",
    "| `owner_zip_code` | String | Owner's zip code |\n",
    "| `latitude` | Float | Latitude of the property |\n",
    "| `longitude` | Float | Longitude of the property |\n",
    "| `violation_number` | String | Violation identifier |\n",
    "\n",
    "*Note: `vacant` column (100% null) is dropped by the pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfit = data_utils.load_unfit_properties()\n",
    "prof_unfit = profile_dataset(unfit, 'Unfit Properties')\n",
    "unfit.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Trash Pickup (~41K rows)\n",
    "**Source:** City of Syracuse - Department of Public Works / Department of Assessment  \n",
    "**Description:** Trash and recycling pickup schedule by individual address for 2025. Lists which day of the week trash is collected and which recycling week (A or B) applies.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `sbl` | String | Section, Block, Lot -- property tax number unique to this property |\n",
    "| `stnum` | String | Parcel street number |\n",
    "| `stname` | String | Parcel street name |\n",
    "| `fulladdres` | String | Full street address (number + name) |\n",
    "| `zip` | Int | Parcel zip code |\n",
    "| `sanitation` | String | Day that trash is picked up on a non-holiday week (Monday-Friday) |\n",
    "| `recyclingw` | String | Which week recycling services are provided: \"A\" or \"B\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = data_utils.load_trash_pickup()\n",
    "prof_trash = profile_dataset(trash, 'Trash Pickup')\n",
    "trash.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Historical Properties (3,486 rows)\n",
    "**Source:** Syracuse Landmark Preservation Board  \n",
    "**Description:** Properties determined to be historic -- locally protected sites and/or National Register of Historic Places eligible. Maintained by Syracuse's Landmark Preservation Board.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `sbl` | String | Section, Block, Lot -- unique parcel identifier |\n",
    "| `property_address` | String | Address of the property or historic district (parcel address, not mailing address) |\n",
    "| `zip` | Int | Zip code of the property or district |\n",
    "| `lpss` | String | Locally Protected Site Status: \"Local Protected Site or Local District\", \"Eligible/Architecturally Significant\", or empty |\n",
    "| `nr_eligible` | String | National Register eligibility: \"NR Listed\" (on the registry), \"NR Eligible (SHPO)\" (eligible via State Historic Preservation Office), or empty (neither) |\n",
    "| `latitude` | Float | Latitude of the property |\n",
    "| `longitude` | Float | Longitude of the property |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical = data_utils.load_historical_properties()\n",
    "prof_historical = profile_dataset(historical, 'Historical Properties')\n",
    "historical.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical deep-dive\n",
    "print(\"National Register eligible:\")\n",
    "if 'nr_eligible' in historical.columns:\n",
    "    print(historical['nr_eligible'].value_counts())\n",
    "print(f\"\\nLPSS (Local Protected Sites):\")\n",
    "if 'lpss' in historical.columns:\n",
    "    print(historical['lpss'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Assessment Roll (~41K rows)\n",
    "**Source:** City of Syracuse - Department of Assessment  \n",
    "**Description:** Tentative tax roll for 2026. Valuation date: Jan 1, 2026. Uniform Percent of Value: 52.00%. Contains property classifications, assessed values, ownership, and tax exemptions for all parcels in Syracuse.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `sbl` | String | Section, Block, Lot -- tax map number unique to this property |\n",
    "| `property_address` | String | Full property address |\n",
    "| `property_city` | String | City, state, and zip code of the property (ZIP extracted by pipeline via regex) |\n",
    "| `dimensions` | String | Width and length of the property in feet, plus features (fireplace, garage, etc.) |\n",
    "| `property_class` | Int | NYS property class code number |\n",
    "| `prop_class_description` | String | Property class description (e.g., \"One Family Year-Round Residence\", \"Apartment\", \"Vacant Land\") |\n",
    "| `primary_owner` | String | Primary owner name or LLC/agency name |\n",
    "| `secondary_owner` | String | Secondary owner name. ~96.3% null |\n",
    "| `owner_address` | String | Primary owner's address (from Assessment records) |\n",
    "| `po_box` | String | Owner PO Box. ~96.2% null |\n",
    "| `school_taxable` | Float | School taxable value |\n",
    "| `municipality_taxable` | Float | Municipality taxable value |\n",
    "| `county_taxable` | Float | County taxable value |\n",
    "| `total_assessment` | Float | Total assessed value of the property (aliased as `total_av` in pipeline schema) |\n",
    "| `exemption_1_description` thru `exemption_6_description` | String | Tax exemption descriptions. 67-100% null; most properties have 0-1 exemptions |\n",
    "| `exemption_1_amt` thru `exemption_6_amt` | Float | Tax exemption amounts |\n",
    "\n",
    "**Pipeline-derived column:** `zip` -- extracted from `property_city` via regex (e.g., \"Syracuse, NY 13205\" -> 13205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment = data_utils.load_assessment_roll()\n",
    "prof_assessment = profile_dataset(assessment, 'Assessment Roll')\n",
    "assessment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment deep-dive\n",
    "print(\"Property class distribution:\")\n",
    "print(assessment['prop_class_description'].value_counts().head(15))\n",
    "if 'total_av' in assessment.columns:\n",
    "    print(f\"\\nTotal assessed value stats:\")\n",
    "    print(assessment['total_av'].describe())\n",
    "    print(f\"\\nAvg assessment by top property classes:\")\n",
    "    top = assessment.groupby('prop_class_description')['total_av'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)\n",
    "    top['mean'] = top['mean'].map('${:,.0f}'.format)\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. SYRCityline Requests (~116K rows)\n",
    "**Source:** City of Syracuse via SeeClickFix software  \n",
    "**Description:** Resident service requests (2021-present) made through SYRCityline. Covers trash pickup, housing maintenance, potholes, street lights, water, sewer, and more. Requests can be submitted via web, mobile app, or phone.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `id` | String | Unique request identifier |\n",
    "| `summary` | String | User-selected category for the complaint (e.g., \"Large or Bulk Items\", \"Weekly Trash Pickup\", \"Home & Building Maintenance\", \"Sewer Back-ups\") |\n",
    "| `rating` | String | Number of followers on the request in SeeClickFix |\n",
    "| `address` | String | Address of the service request, provided by the community member |\n",
    "| `description` | String | Write-up of the request/complaint, provided by the community member |\n",
    "| `agency_name` | String | City department assigned (e.g., \"Garbage, Recycling & Graffiti\", \"Housing & Property Maintenance\", \"Streets, Sidewalks & Transportation\") |\n",
    "| `request_type` | Float | Numeric request type code |\n",
    "| `latitude` | Float | Latitude (renamed from `lat` by pipeline) |\n",
    "| `longitude` | Float | Longitude (renamed from `lng` by pipeline) |\n",
    "| `created_at_local` | Datetime | When the request was submitted (format: MM/DD/YYYY - HH:MM AM/PM) |\n",
    "| `acknowledged_at_local` | Datetime | When the request was acknowledged by the city department. ~80.6% null |\n",
    "| `closed_at_local` | Datetime | When the request was marked closed |\n",
    "| `minutes_to_acknowledge` | Float | Minutes from creation to acknowledgment. ~80.6% null |\n",
    "| `minutes_to_close` | Float | Minutes from creation to closure. Median ~22 hrs, max ~2.7 years |\n",
    "| `assignee_name` | String | City department assigned to the request |\n",
    "| `category` | String | Request category (e.g., \"Potholes\", \"Street Lights\", \"Weekly Trash Pickup\", \"Water-related Concerns\") |\n",
    "| `sla_in_hours` | Float | Service Level Agreement limit in hours -- how long before escalation to department head |\n",
    "| `report_source` | String | How the request was submitted: \"Web-Mobile\", \"iPhone\", \"Portal\", \"Web-Desktop\", \"Android\", etc. |\n",
    "\n",
    "**Pipeline-derived columns:** `zip` (from lat/long), `year` (from `created_at_local`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityline = data_utils.load_cityline_requests()\n",
    "prof_cityline = profile_dataset(cityline, 'SYRCityline Requests')\n",
    "cityline.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityline deep-dive\n",
    "print(\"Top categories:\")\n",
    "print(cityline['category'].value_counts().head(15))\n",
    "print(f\"\\nTop agencies:\")\n",
    "print(cityline['agency_name'].value_counts().head(10))\n",
    "print(f\"\\nReport sources:\")\n",
    "if 'report_source' in cityline.columns:\n",
    "    print(cityline['report_source'].value_counts())\n",
    "if 'minutes_to_close' in cityline.columns:\n",
    "    print(f\"\\nMinutes to close:\")\n",
    "    print(cityline['minutes_to_close'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Snow Routes (3,685 rows)\n",
    "**Source:** City of Syracuse; compiled by Syracuse Community Geography (Sept 2017)  \n",
    "**Description:** Street segments listed as emergency snow routes in the City Charter (Sec. 15-337). During a declared snow emergency, parking is unlawful on these streets. Derived from a 2016 NYS GIS Clearinghouse street segment file. For reference and planning only.\n",
    "\n",
    "### Column Dictionary (key columns out of ~70 total)\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `completest` | String | Complete street name |\n",
    "| `streetname` | String | Core street name (without prefix/suffix directions) |\n",
    "| `predirecti` | String | Pre-direction (N, S, E, W) |\n",
    "| `posttype` | String | Street suffix type (St, Ave, Blvd, etc.) |\n",
    "| `postdirect` | String | Post-direction |\n",
    "| `leftpostal` | String | Left-side postal/ZIP code (used by pipeline as `zip`) |\n",
    "| `rightposta` | String | Right-side postal/ZIP code |\n",
    "| `speed` | Int | Speed limit |\n",
    "| `oneway` | String | One-way designation |\n",
    "| `leftfromad` / `lefttoaddr` | String | Left-side address range (from/to) |\n",
    "| `rightfroma` / `righttoadd` | String | Right-side address range (from/to) |\n",
    "| `label` | String | Display label for the route |\n",
    "| `leftcensus` / `rightcensu` | String | Census tract on left/right side |\n",
    "| `shape_leng` / `shape_stle` | Float | Geometry length |\n",
    "\n",
    "*Remaining ~55 columns are NYS GIS street segment standard fields (road metadata, jurisdictions, edit timestamps, etc.). No official descriptions provided on the portal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = data_utils.load_snow_routes()\n",
    "prof_snow = profile_dataset(snow, 'Snow Routes')\n",
    "snow.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Bike Suitability (868 rows)\n",
    "**Source:** Syracuse Metropolitan Transportation Council (SMTC); ratings collected by staff in 2019  \n",
    "**Description:** Road suitability ratings for bicycle travel in Greater Syracuse. Ratings: **Excellent** (slow/low traffic, some separation from vehicles), **Good** (moderate traffic, some separation), **Fair/Urban Acceptable** (moderate traffic, little separation or higher volumes with some separation), **Poor** (heavy/fast traffic, little/no separation, rough conditions). For planning purposes only.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `name` | String | Name of the road or road segment |\n",
    "| `bike_suitability_19` | String | Suitability rating: \"Excellent: Highly Recommend\", \"Good: Acceptable\", \"Fair: Urban Acceptable\", or \"Poor: Not suitable\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_suit = data_utils.load_bike_suitability()\n",
    "prof_bike_suit = profile_dataset(bike_suit, 'Bike Suitability')\n",
    "bike_suit.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Bike Infrastructure (59 rows)\n",
    "**Source:** Syracuse Metropolitan Transportation Council (SMTC)  \n",
    "**Description:** Overview of bicycle infrastructure in Syracuse as of 2023. Includes designated bike lanes, bikeways, trails, and sharrows. Not exhaustive -- infrastructure may have been installed since the last update. Some trail portions may extend outside city limits.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `infrastructure_type` | String | Type of bike infrastructure: \"Lanes\", \"Bikeway\", \"Trails\", or \"Sharrows\" |\n",
    "| `trail_name` | String | Name of the bike trail (if applicable) |\n",
    "| `length_mi` | Float | Length of this section of bike infrastructure, in miles |\n",
    "| `globalid` | String | ArcGIS system field |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_infra = data_utils.load_bike_infrastructure()\n",
    "prof_bike_infra = profile_dataset(bike_infra, 'Bike Infrastructure')\n",
    "bike_infra.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Parking Violations (~197K rows)\n",
    "**Source:** Parking Violations Bureau  \n",
    "**Description:** Parking tickets issued in Syracuse since 2023. Largest dataset in the project. Note from the portal: many addresses don't map correctly; table view is more complete for tallies.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `ticket_number` | String | Ticket number -- listed on the physical parking ticket and used for internal tracking |\n",
    "| `issued_date` | Date | Date and time the parking ticket was issued (stored as epoch milliseconds) |\n",
    "| `location` | String | Block where the ticket was given (typically rounded to 100-block level) |\n",
    "| `description` | String | The parking regulation alleged to have been violated |\n",
    "| `status` | String | Payment status: \"Paid in Full\", \"Collections\", \"Adjudication Approved\", \"Promise to Pay\", \"Adjudication Denied\", \"Hearing Appeal Affirmed\", \"Adjudication Partial\", \"Paid in Collections\", \"In Bankruptcy\", or \"Administrative Review PIF\" |\n",
    "| `amount` | Float | Fine amount in dollars |\n",
    "| `latitude` | Float | Latitude (renamed from `lat` by pipeline) |\n",
    "| `longitude` | Float | Longitude (renamed from `long` by pipeline) |\n",
    "\n",
    "**Pipeline-derived columns:** `zip` (from lat/long), `year` (from `issued_date`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking = data_utils.load_parking_violations()\n",
    "prof_parking = profile_dataset(parking, 'Parking Violations')\n",
    "parking.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parking deep-dive\n",
    "print(\"Top violation types:\")\n",
    "print(parking['description'].value_counts().head(15))\n",
    "print(f\"\\nStatus:\")\n",
    "print(parking['status'].value_counts())\n",
    "if 'amount' in parking.columns:\n",
    "    print(f\"\\nFine amounts:\")\n",
    "    print(parking['amount'].describe())\n",
    "    print(f\"\\nAvg fine by top violation types:\")\n",
    "    avg_fines = parking.groupby('description')['amount'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)\n",
    "    avg_fines['mean'] = avg_fines['mean'].map('${:.2f}'.format)\n",
    "    print(avg_fines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Permit Requests (~47K rows)\n",
    "**Source:** Central Permit Office  \n",
    "**Description:** Building permits filed with the Central Permit Office. *Portal note (11/12/2025): This dataset is not currently updating correctly and does not reflect current permit statistics.*\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `permit_number` | String | Permit number assigned by the Central Permit Office |\n",
    "| `full_address` | String | Full property address |\n",
    "| `owner` | String | Property owner name |\n",
    "| `issue_date` | Date | Date the permit was issued (stored as epoch milliseconds) |\n",
    "| `permit_type` | String | Type of permit (e.g., \"Building Permit\", \"Electrical\", \"Plumbing\") |\n",
    "| `description_of_work` | String | Description of the work being done (29,283 unique free-text values) |\n",
    "| `latitude` | Float | Latitude (renamed from `lat` by pipeline) |\n",
    "| `longitude` | Float | Longitude (renamed from `long` by pipeline) |\n",
    "\n",
    "**Pipeline-derived columns:** `zip` (from lat/long), `year` (from `issue_date`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permits = data_utils.load_permit_requests()\n",
    "prof_permits = profile_dataset(permits, 'Permit Requests')\n",
    "permits.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permits deep-dive\n",
    "print(\"Permit types:\")\n",
    "print(permits['permit_type'].value_counts().head(15))\n",
    "if 'zip' in permits.columns:\n",
    "    print(f\"\\nTop ZIP codes:\")\n",
    "    print(permits['zip'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Tree Inventory (~55K rows)\n",
    "**Source:** City of Syracuse Parks & Recreation Forestry staff and Cornell Cooperative Extension's CommuniTree Stewards  \n",
    "**Description:** Inventory of city-managed trees in Syracuse. Entered by Parks & Recreation Forestry staff and the CommuniTree Stewards program.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `id` | Float | Internal numerical identifier for the tree site |\n",
    "| `spp_com` | String | Tree species -- common English name (e.g., \"Norway Maple\", \"Red Oak\") |\n",
    "| `spp_bot` | String | Tree species -- Latin/botanical name |\n",
    "| `dbh` | Float | Diameter at Breast Height -- standard tree measurement at 4.5 feet above ground, in inches |\n",
    "| `area` | String | Area/neighborhood of the city the tree is in (uses the city's neighborhoods map). Serves as the `neighborhood` equivalent for this dataset |\n",
    "| `plantdate` | String | Season and year the tree was planted (if known); may be \"N/A\" or \"To Be Determined\" |\n",
    "| `censustrac` | String | Census tract the tree is in (56 unique tracts) |\n",
    "| `arpa` | String | Whether the tree was planted with American Rescue Plan Act (ARPA) funds (\"Yes\" ~55.8%, \"No\" ~44.2%) |\n",
    "| `address` | Float | Street number closest to the tree |\n",
    "| `street` | String | Street the tree is located on |\n",
    "| `x`, `y` | Float | Map projection coordinates |\n",
    "| `latitude` | Float | Latitude |\n",
    "| `longitude` | Float | Longitude |\n",
    "\n",
    "**Pipeline-derived column:** `zip` (from lat/long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = data_utils.load_tree_inventory()\n",
    "prof_trees = profile_dataset(trees, 'Tree Inventory')\n",
    "trees.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees deep-dive\n",
    "print(\"NOTE: Tree inventory has NO 'condition' column (schema references it but it doesn't exist)\")\n",
    "print(\"'area' column = neighborhood equivalent\\n\")\n",
    "print(f\"Top species:\")\n",
    "if 'spp_com' in trees.columns:\n",
    "    print(trees['spp_com'].value_counts().head(15))\n",
    "print(f\"\\nTrees by area/neighborhood (top 10):\")\n",
    "if 'area' in trees.columns:\n",
    "    print(trees['area'].value_counts().head(10))\n",
    "print(f\"\\nARPA (American Rescue Plan Act) trees:\")\n",
    "if 'arpa' in trees.columns:\n",
    "    print(trees['arpa'].value_counts())\n",
    "print(f\"\\nDiameter (DBH) stats:\")\n",
    "if 'dbh' in trees.columns:\n",
    "    print(trees['dbh'].describe())\n",
    "print(f\"\\nCensus tracts: {trees['censustrac'].nunique() if 'censustrac' in trees.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Lead Testing (1,185 rows)\n",
    "**Source:** City of Syracuse (distributed as Excel files, not on ArcGIS portal)  \n",
    "**Description:** Elevated blood lead levels in children by census tract (2013-2024). Raw data is wide format (tracts as rows, years as columns) with 3 header rows; pipeline melts to long format. Values may be \"Suppressed\" for small sample sizes (privacy protection), converted to NaN. No official portal data dictionary exists for this dataset.\n",
    "\n",
    "### Column Dictionary\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `census_tract` | String | 2020 Census Tract identifier |\n",
    "| `year` | Int (nullable) | Year of the lead testing data (2013-2024) |\n",
    "| `pct_elevated` | Float | Percentage of children tested with elevated blood lead levels. \"Suppressed\" values converted to NaN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead = data_utils.load_lead_testing()\n",
    "prof_lead = profile_dataset(lead, 'Lead Testing')\n",
    "lead.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead deep-dive\n",
    "print(\"Year distribution:\")\n",
    "print(lead['year'].value_counts().sort_index())\n",
    "print(f\"\\nCensus tracts: {lead['census_tract'].nunique()}\")\n",
    "if 'pct_elevated' in lead.columns:\n",
    "    print(f\"\\nPercent elevated stats:\")\n",
    "    print(lead['pct_elevated'].describe())\n",
    "    print(f\"\\nTracts with highest avg elevated lead:\")\n",
    "    top_lead = lead.groupby('census_tract')['pct_elevated'].mean().sort_values(ascending=False).head(10)\n",
    "    print(top_lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: All Datasets at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table\n",
    "all_datasets = {\n",
    "    'Code Violations': violations,\n",
    "    'Crime': crime,\n",
    "    'Rental Registry': rentals,\n",
    "    'Vacant Properties': vacant,\n",
    "    'Unfit Properties': unfit,\n",
    "    'Trash Pickup': trash,\n",
    "    'Historical Properties': historical,\n",
    "    'Assessment Roll': assessment,\n",
    "    'Cityline Requests': cityline,\n",
    "    'Snow Routes': snow,\n",
    "    'Bike Suitability': bike_suit,\n",
    "    'Bike Infrastructure': bike_infra,\n",
    "    'Parking Violations': parking,\n",
    "    'Permit Requests': permits,\n",
    "    'Tree Inventory': trees,\n",
    "    'Lead Testing': lead,\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, df in all_datasets.items():\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n",
    "    has_geo = any(c in df.columns for c in ['latitude', 'longitude', 'lat', 'long', 'x', 'y'])\n",
    "    has_zip = 'zip' in df.columns or 'complaint_zip' in df.columns\n",
    "    has_neighborhood = 'neighborhood' in df.columns\n",
    "    null_pct = df.isna().mean().mean() * 100\n",
    "    \n",
    "    summary.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': f\"{len(df):,}\",\n",
    "        'Cols': len(df.columns),\n",
    "        'Memory (MB)': f\"{df.memory_usage(deep=True).sum()/1024**2:.1f}\",\n",
    "        'Numeric Cols': len(num_cols),\n",
    "        'Date Cols': len(date_cols),\n",
    "        'Has Geo': '✓' if has_geo else '',\n",
    "        'Has ZIP': '✓' if has_zip else '',\n",
    "        'Has Neighborhood': '✓' if has_neighborhood else '',\n",
    "        'Avg Null %': f\"{null_pct:.1f}%\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"ALL 16 DATASETS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_df)\n",
    "\n",
    "total_rows = sum(len(df) for df in all_datasets.values())\n",
    "total_mem = sum(df.memory_usage(deep=True).sum() for df in all_datasets.values()) / 1024**2\n",
    "print(f\"\\nTotal: {total_rows:,} rows across 16 datasets, {total_mem:.1f} MB in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross-Dataset Join Potential\n",
    "Which datasets can be linked together and how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check join keys across datasets\n",
    "print(\"JOIN KEY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# SBL joins (property-level)\n",
    "sbl_datasets = {name: df for name, df in all_datasets.items() if 'sbl' in df.columns}\n",
    "print(f\"\\nDatasets with SBL (property ID), enables property-level joins:\")\n",
    "for name, df in sbl_datasets.items():\n",
    "    print(f\"  {name}: {df['sbl'].nunique():,} unique SBLs\")\n",
    "\n",
    "# ZIP joins\n",
    "zip_datasets = {}\n",
    "for name, df in all_datasets.items():\n",
    "    if 'zip' in df.columns:\n",
    "        zip_datasets[name] = df['zip']\n",
    "    elif 'complaint_zip' in df.columns:\n",
    "        zip_datasets[name] = df['complaint_zip']\n",
    "print(f\"\\nDatasets with ZIP code, enables area-level joins:\")\n",
    "for name, series in zip_datasets.items():\n",
    "    print(f\"  {name}: {series.nunique()} unique ZIPs\")\n",
    "\n",
    "# Neighborhood joins\n",
    "nbhd_datasets = {name: df for name, df in all_datasets.items() if 'neighborhood' in df.columns}\n",
    "print(f\"\\nDatasets with Neighborhood, enables neighborhood-level joins:\")\n",
    "for name, df in nbhd_datasets.items():\n",
    "    print(f\"  {name}: {df['neighborhood'].nunique()} unique neighborhoods\")\n",
    "\n",
    "# Check neighborhood overlap\n",
    "if len(nbhd_datasets) >= 2:\n",
    "    names = list(nbhd_datasets.keys())\n",
    "    print(f\"\\nNeighborhood overlap between {names[0]} and {names[1]}:\")\n",
    "    set1 = set(nbhd_datasets[names[0]]['neighborhood'].dropna().unique())\n",
    "    set2 = set(nbhd_datasets[names[1]]['neighborhood'].dropna().unique())\n",
    "    overlap = set1 & set2\n",
    "    print(f\"  {names[0]}: {len(set1)} | {names[1]}: {len(set2)} | Overlap: {len(overlap)}\")\n",
    "    only1 = set1 - set2\n",
    "    only2 = set2 - set1\n",
    "    if only1:\n",
    "        print(f\"  Only in {names[0]}: {sorted(only1)[:10]}\")\n",
    "    if only2:\n",
    "        print(f\"  Only in {names[1]}: {sorted(only2)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality check across all datasets\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for name, df in all_datasets.items():\n",
    "    # Check for duplicate rows\n",
    "    dupes = df.duplicated().sum()\n",
    "    if dupes > 0:\n",
    "        quality_issues.append(f\"  {name}: {dupes:,} duplicate rows ({dupes/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for high-null columns (>50%)\n",
    "    for col in df.columns:\n",
    "        null_pct = df[col].isna().mean() * 100\n",
    "        if null_pct > 50:\n",
    "            quality_issues.append(f\"  {name}.{col}: {null_pct:.0f}% null\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(\"\\nIssues found:\")\n",
    "    for issue in quality_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"  No major quality issues found!\")\n",
    "\n",
    "# Check for common column name inconsistencies\n",
    "print(\"\\nCOLUMN NAME CONSISTENCY:\")\n",
    "all_cols = set()\n",
    "for name, df in all_datasets.items():\n",
    "    all_cols.update(df.columns)\n",
    "# Check for near-duplicates\n",
    "col_list = sorted(all_cols)\n",
    "print(f\"  Total unique columns across all datasets: {len(col_list)}\")\n",
    "print(f\"  Geographic columns found: {[c for c in col_list if c in ['zip', 'complaint_zip', 'latitude', 'longitude', 'lat', 'long', 'x', 'y', 'neighborhood', 'census_tract']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What Can We Do With This Data?\n",
    "\n",
    "## Currently Supported Queries\n",
    "- Counts, averages, min/max/sum by group\n",
    "- Temporal breakdowns (year, month, quarter)\n",
    "- Cross-dataset joins (violations + rentals, crime + vacant, etc.)\n",
    "- Rankings, percentiles via LLM SQL path\n",
    "\n",
    "## New Analysis Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify new analysis opportunities based on the data\n",
    "\n",
    "print(\"ANALYSIS OPPORTUNITIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. HOUSING HEALTH INDEX\n",
    "   Combine: violations + vacant + unfit + rental_registry + assessment\n",
    "   By: neighborhood or ZIP\n",
    "   Metric: composite score of property distress\n",
    "   Data available: All 5 datasets have ZIP or neighborhood\n",
    "\n",
    "2. CRIME + PROPERTY CONDITIONS CORRELATION\n",
    "   Combine: crime + violations + vacant_properties\n",
    "   Question: \"Do neighborhoods with more vacant properties have more crime?\"\n",
    "   Data available: All have neighborhood column\n",
    "\n",
    "3. CITY RESPONSIVENESS ANALYSIS\n",
    "   Dataset: cityline_requests (minutes_to_close)\n",
    "   Questions: Which categories get resolved fastest? Which ZIPs wait longest?\n",
    "   Data available: 116K requests with response times\n",
    "\n",
    "4. LEAD EXPOSURE + HOUSING CONDITIONS\n",
    "   Combine: lead_testing + violations (would need census tract to neighborhood/ZIP mapping)\n",
    "   Question: \"Do areas with more code violations have higher lead levels?\"\n",
    "   Gap: lead_testing uses census_tract, others use ZIP/neighborhood\n",
    "\n",
    "5. TEMPORAL TRENDS (Multi-Year)\n",
    "   Datasets with year: crime (2022-2025), violations (2017+), cityline, permits, parking\n",
    "   Questions: \"Is crime going up or down?\" \"Violation trends over time?\"\n",
    "   Data available: Yes\n",
    "\n",
    "6. TREE CANOPY + NEIGHBORHOOD QUALITY\n",
    "   Combine: tree_inventory + violations + crime\n",
    "   Question: \"Do neighborhoods with more trees have fewer violations?\"\n",
    "   Data available: All have neighborhood\n",
    "\n",
    "7. PROPERTY VALUE ANALYSIS\n",
    "   Dataset: assessment_roll (total_av by prop_class, ZIP)\n",
    "   Combined with: violations, vacant to see if distressed areas have lower assessments\n",
    "   Data available: Yes\n",
    "\n",
    "8. GEOGRAPHIC HOTSPOT MAPPING\n",
    "   Datasets with lat/long: crime, cityline, parking_violations\n",
    "   Opportunity: heatmaps, clustering, density analysis\n",
    "   Currently: only bubble maps and point maps\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Column Inventory (Every Column in Every Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full column inventory for reference\n",
    "print(\"FULL COLUMN INVENTORY\")\n",
    "print(\"=\" * 70)\n",
    "for name, df in all_datasets.items():\n",
    "    print(f\"\\n{name} ({len(df):,} rows):\")\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        nunique = df[col].nunique()\n",
    "        sample = str(df[col].dropna().iloc[0])[:50] if df[col].notna().any() else 'ALL NULL'\n",
    "        print(f\"  {col:35s} {dtype:15s} {nunique:>6} unique  sample: {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EDA Findings & Inconsistencies\n",
    "\n",
    "## Dataset Overview (691,827 total rows, 387.6 MB in memory)\n",
    "\n",
    "| Dataset | Rows | Cols | Geo | ZIP | Neighborhood | SBL | Year |\n",
    "|---|---:|---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Code Violations | 137,663 | 25 | Y | Y | Y | Y | |\n",
    "| Crime | 32,840 | 17 | Y | Y | Y | | Y |\n",
    "| Rental Registry | 11,085 | 22 | Y | Y | | Y | |\n",
    "| Vacant Properties | 1,651 | 17 | Y | Y | Y | Y | |\n",
    "| Unfit Properties | 353 | 27 | Y | Y | | Y | |\n",
    "| Trash Pickup | 41,096 | 7 | | Y | | Y | |\n",
    "| Historical Properties | 3,486 | 7 | Y | Y | | Y | |\n",
    "| Assessment Roll | 41,367 | 27 | | Y | | Y | |\n",
    "| Cityline Requests | 116,143 | 19 | Y | Y | | | Y |\n",
    "| Snow Routes | 3,685 | 70 | | Y | | | |\n",
    "| Bike Suitability | 868 | 2 | | | | | |\n",
    "| Bike Infrastructure | 59 | 4 | | | | | |\n",
    "| Parking Violations | 196,768 | 9 | Y | Y | | | Y |\n",
    "| Permit Requests | 47,902 | 9 | Y | Y | | | Y |\n",
    "| Tree Inventory | 55,676 | 15 | Y | Y | | | |\n",
    "| Lead Testing | 1,185 | 3 | | | | | Y |\n",
    "\n",
    "---\n",
    "\n",
    "## SBL (Property-Level) Join Overlap\n",
    "\n",
    "7 datasets share SBL. Strongest overlaps:\n",
    "\n",
    "| Dataset A | Dataset B | Shared SBLs | A total | B total | Match % (of smaller) |\n",
    "|---|---|---:|---:|---:|---:|\n",
    "| Code Violations | Assessment Roll | 17,074 | 17,348 | 41,367 | 98.4% |\n",
    "| Code Violations | Trash Pickup | 17,192 | 17,348 | 40,865 | 99.1% |\n",
    "| Code Violations | Rental Registry | 7,991 | 17,348 | 11,048 | 72.3% |\n",
    "| Rental Registry | Assessment Roll | 11,028 | 11,048 | 41,367 | 99.8% |\n",
    "| Code Violations | Vacant Properties | 1,483 | 17,348 | 1,651 | 89.8% |\n",
    "| Vacant Properties | Assessment Roll | 1,632 | 1,651 | 41,367 | 98.8% |\n",
    "| Vacant Properties | Unfit Properties | 101 | 1,651 | 305 | 33.1% |\n",
    "| Trash Pickup | Assessment Roll | 40,535 | 40,865 | 41,367 | 99.2% |\n",
    "| Historical Properties | Assessment Roll | 3,439 | 3,471 | 41,367 | 99.1% |\n",
    "\n",
    "Assessment Roll is the universal backbone -- nearly every property in other datasets matches it.\n",
    "\n",
    "## Neighborhood Join Overlap\n",
    "\n",
    "Only 3 datasets have `neighborhood`: Code Violations (34), Crime (34), Vacant Properties (33).\n",
    "\n",
    "- Violations + Crime: 27/41 neighborhoods match\n",
    "- Violations + Vacant: 33/34 match\n",
    "- Crime + Vacant: 27/40 match\n",
    "\n",
    "Tree Inventory uses `area` instead of `neighborhood` (same concept, 31 areas). Schema already correctly uses `area`.\n",
    "\n",
    "## ZIP Codes\n",
    "\n",
    "12 Syracuse ZIPs across 12/16 datasets: 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13210, 13214, 13215, 13219, 13224.\n",
    "\n",
    "Datasets without ZIP: Bike Suitability, Bike Infrastructure, Lead Testing (uses census_tract).\n",
    "\n",
    "---\n",
    "\n",
    "## Data Quality Issues Found\n",
    "\n",
    "### Duplicate Rows (FIXED)\n",
    "- **Trash Pickup**: 221 duplicate rows (0.5%) -- FIXED: added `.drop_duplicates()` in loader\n",
    "- **Cityline Requests**: 835 duplicate rows (0.7%) -- FIXED: added `.drop_duplicates()` in loader\n",
    "- **Historical Properties**: 15 duplicate rows (0.4%) -- FIXED: added `.drop_duplicates()` in loader\n",
    "- **Permit Requests**: 1 duplicate row (negligible, not fixed)\n",
    "\n",
    "### High-Null Columns (>50%)\n",
    "\n",
    "| Dataset | Column | Null % | Notes |\n",
    "|---|---|---:|---|\n",
    "| Code Violations | `status_date` | 88.6% | Only populated for closed violations |\n",
    "| Code Violations | `vacant` | 86.2% | Only \"Residential\"/\"Commercial\" when flagged |\n",
    "| Crime | `arrest` | 86.9% | Was only \"Yes\" values -- FIXED: nulls now filled with \"No\" |\n",
    "| Crime | `larcenycode` | 53.7% | Only applies to larceny offenses |\n",
    "| Crime | `qualityoflife` | 59.2% | Only for 2023+ data |\n",
    "| Rental Registry | `rr_ext_insp_fail` | 94.6% | Only populated on failure |\n",
    "| Rental Registry | `rr_int_insp_fail` | 92.2% | Only populated on failure |\n",
    "| Rental Registry | `rr_ext_insp_pass` | 56.0% | Only populated on pass |\n",
    "| Rental Registry | `rr_int_insp_pass` | 59.1% | Only populated on pass |\n",
    "| Rental Registry | `shape` | 100.0% | FIXED: dropped in loader (GIS artifact) |\n",
    "| Vacant Properties | `completion_date` | 77.3% | Only for VPR-certified properties |\n",
    "| Unfit Properties | `vacant` | 100.0% | FIXED: dropped in loader |\n",
    "| Assessment Roll | `secondary_owner` | 96.3% | Rare |\n",
    "| Assessment Roll | `po_box` | 96.2% | Rare |\n",
    "| Assessment Roll | `exemption_1_*` through `exemption_6_*` | 67-100% | Most properties have 0-1 exemptions |\n",
    "| Cityline Requests | `acknowledged_at_local` | 80.6% | Most requests never formally acknowledged |\n",
    "| Cityline Requests | `minutes_to_acknowledge` | 80.6% | Same as above |\n",
    "\n",
    "### Schema vs Reality Mismatches\n",
    "\n",
    "| Issue | Status |\n",
    "|---|---|\n",
    "| Tree `condition`/`neighborhood` columns | NOT A BUG: schema.py already correctly uses `area` and `dbh`. Only CLAUDE.md was wrong -- FIXED in CLAUDE.md. |\n",
    "| Unfit `vacant` column is 100% null | FIXED: column dropped in loader |\n",
    "| Rental Registry `shape` column is 100% null | FIXED: column dropped in loader |\n",
    "| Crime `arrest` has no \"No\" values | FIXED: null handling changed from \"Unknown\" to \"No\" |\n",
    "| Code Violations actual rows: 137,663 | FIXED: README updated from \"~44K\" to \"~138K\" |\n",
    "| Cityline/Parking/Permits missing extracted `year` column | FIXED: year extracted from date columns in all 3 loaders |\n",
    "\n",
    "---\n",
    "\n",
    "## Unexposed Columns Worth Adding\n",
    "\n",
    "### High Value (would enable new query types)\n",
    "\n",
    "| Dataset | Column | Values | Enables |\n",
    "|---|---|---|---|\n",
    "| Crime | `larcenycode` | 12 types (All Other, From Building, Shoplifting, etc.) | \"What types of larceny are most common?\" |\n",
    "| Crime | `arrest` | Yes (4,315) / No (28,525) | \"How many crimes resulted in arrest?\" |\n",
    "| Cityline | `sla_in_hours` | 17 values (0-2605 hrs) | \"Which request types have the longest SLA?\" |\n",
    "| Cityline | `summary` | 266 types | More granular than `category` |\n",
    "| Tree Inventory | `censustrac` | 56 census tracts | Could link to lead testing data |\n",
    "| Tree Inventory | `arpa` | Yes (55.8%) / No (44.2%) | \"How many ARPA-funded trees?\" |\n",
    "| Assessment Roll | `school_taxable` | Varies by exemptions | Tax analysis |\n",
    "\n",
    "### Medium Value\n",
    "\n",
    "| Dataset | Column | Notes |\n",
    "|---|---|---|\n",
    "| Crime | `qualityoflife` | True/False, but 59% null (only 2023+ data) |\n",
    "| Vacant Properties | `vacant` | Residential (90.2%) vs Commercial (9.8%) |\n",
    "| Rental Registry | `rrisvalid` | Yes (40.8%) / No (59.2%) -- current validity |\n",
    "| Unfit Properties | `corrective_action` | 249 unique text descriptions |\n",
    "| Parking Violations | `location` | 8,874 unique locations |\n",
    "| Permit Requests | `description_of_work` | 29,283 unique -- free text, hard to group |\n",
    "\n",
    "---\n",
    "\n",
    "## New Cross-Dataset Analysis Opportunities\n",
    "\n",
    "1. **Housing Health Index** -- Combine violations + vacant + unfit + rental_registry + assessment by neighborhood/ZIP into a composite distress score. All 5 datasets share SBL or ZIP.\n",
    "\n",
    "2. **Crime vs Property Conditions** -- Do neighborhoods with more vacant properties have more crime? All 3 have neighborhood. 27/41 neighborhoods overlap between crime and violations.\n",
    "\n",
    "3. **City Responsiveness** -- Cityline has 116K requests with `minutes_to_close`. Which categories resolve fastest? Which ZIPs wait longest? Median close time is 1,327 minutes (~22 hours), max is 1,408,207 minutes (~2.7 years).\n",
    "\n",
    "4. **Lead Exposure vs Housing** -- Lead testing uses census_tract (157 tracts). Tree inventory also has `censustrac` (56 tracts). Could bridge lead data to property data through census tracts.\n",
    "\n",
    "5. **Tree Canopy vs Neighborhood Quality** -- Tree inventory `area` column maps to neighborhoods. Compare tree density/diameter against violations and crime rates.\n",
    "\n",
    "6. **Property Value vs Distress** -- Assessment roll has property values for 98.8% of vacant properties (via SBL). Compare assessed values for vacant vs non-vacant, violation-heavy vs clean.\n",
    "\n",
    "7. **Temporal Trends** -- Crime (2022-2025), violations (2017+), permits, cityline all have date columns. Year-over-year trend analysis. Note: 2025 crime data is only 33 rows (Jan 1-5).\n",
    "\n",
    "8. **Geographic Hotspot Mapping** -- 9 datasets have lat/long. Density heatmaps, clustering, hotspot detection. Currently only bubble and point maps in the app.\n",
    "\n",
    "---\n",
    "\n",
    "## All Fixes Applied\n",
    "\n",
    "| # | Fix | What Changed | File(s) Modified |\n",
    "|---|---|---|---|\n",
    "| 1 | Pre-merged crime CSV | Created `crime_merged.csv` (32,840 rows) with ZIP, neighborhood, year, crime_part pre-computed. Eliminates geocoding on every load. 1.8x faster. | `data/raw/crime_merged.csv` |\n",
    "| 2 | `load_crime()` uses merged file | Fast path reads single CSV. Falls back to 6-CSV + geocoding if missing. Also applies null handling to merged path. | `pipeline/data_utils.py` |\n",
    "| 3 | README violation count | Changed \"~44K\" to \"~138K\" (137,663 violation rows). Also fixed vacant from \"~1.4K\" to \"~1.7K\". | `README.md` |\n",
    "| 4 | Trash Pickup duplicates | Added `.drop_duplicates()` in loader. Removes 221 duplicate rows (41,096 -> 40,875). | `pipeline/data_utils.py` |\n",
    "| 5 | Cityline duplicates | Added `.drop_duplicates()` in loader. Removes 835 duplicate rows (116,143 -> 115,308). | `pipeline/data_utils.py` |\n",
    "| 6 | Historical Properties duplicates | Added `.drop_duplicates()` in loader. Removes 15 duplicate rows (3,486 -> 3,471). | `pipeline/data_utils.py` |\n",
    "| 7 | Rental Registry `shape` column | Dropped 100% null column in loader. Saves memory. 22 -> 21 columns. | `pipeline/data_utils.py` |\n",
    "| 8 | Unfit Properties `vacant` column | Dropped 100% null column in loader. 27 -> 26 columns. | `pipeline/data_utils.py` |\n",
    "| 9 | Crime `arrest` null values | Changed null handling label from \"Unknown\" to \"No\". Now shows Yes (4,315) / No (28,525) instead of Yes/null. Regenerated crime_merged.csv. | `pipeline/data_utils.py`, `data/raw/crime_merged.csv` |\n",
    "| 10 | Year extraction for 3 datasets | Added `year = dt.year` extraction in loaders for Cityline (from `created_at_local`, 2021-2025), Parking Violations (from `issued_date`, 2023-2025), and Permit Requests (from `issue_date`, 1980-2025). Schema already had `year` in `temporal_group_map` but the column didn't exist at load time. | `pipeline/data_utils.py` |\n",
    "| 11 | CLAUDE.md tree_inventory entry | Fixed \"neighborhood, condition\" to \"area, spp_com, zip; avg/min/max dbh\" to match actual schema and data. | `CLAUDE.md` |\n",
    "\n",
    "All 13 benchmark tests pass after fixes (100%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
